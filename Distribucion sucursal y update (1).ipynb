{"cells":[{"cell_type":"code","source":["import pyspark\nfrom pyspark.sql import SparkSession\nimport pandas as pd\nimport numpy as np\nimport pyodbc\nspark=SparkSession.builder.appName(\"transformaciones\").getOrCreate()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"77f0d13d-d23e-4c69-a339-918e87937a64","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["datalake='datalakemarcos'\ncontainer='productosjson'\nAzureSQL='sqlmarcos'\nAccesKey='JnVoTznnKD9tBmycYhwhulbkpnIWunlvTuC572T+c4O/nF3gsDrSIuFZy1/Lrr9TlYHYBv6yvJ8N+AStduQj7A=='\nbacpac='dbRetail'\nuser='server'\npss='Test1234'"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"236d21ab-5c38-45d0-9f66-4d14925568de","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["dbutils.fs.mount(\n    source = 'wasbs://'+container+'@'+datalake+'.blob.core.windows.net',\n    mount_point = '/mnt/'+container,\n    extra_configs = {'fs.azure.account.key.'+datalake+'.blob.core.windows.net':AccesKey}\n)\n#creo el punto de montaje con el datalake"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"9951064b-e384-462b-aad6-46ced53454ca","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["jdbcHostname = AzureSQL+\".database.windows.net\"\njdbcPort = 1433\njdbcDatabase = bacpac\njdbcUsername = user\njdbcPassword = \"Test1234\"\njdbcDriver = \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n\njdbcUrl = f\"jdbc:sqlserver://{jdbcHostname}:{jdbcPort};databaseName={jdbcDatabase};user={jdbcUsername};password={jdbcPassword}\"\n\n#conexion al azure sql"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"7da18014-8193-4bcd-aac1-cc603bbccdc4","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["jsonProductos=spark.read.option('multiline','true').json('/mnt/output/salida.json')\njsonProductos.show()\n#LEO EL JSON DEL PROVEEDOR ^^^^^^\n\njsonProductospd=jsonProductos.toPandas()\ncolumnas_json=jsonProductospd.to_numpy().transpose().tolist()\nlista_cod_producto=[int(i) for i in columnas_json[0]]\nlista_stock=[int(i) for i in columnas_json[1]]\n#CREO LISTAS CON LAS COLUMNAS DEL DATAFRAME ^^^^^^\n#LAS VOY A METER A UNA FUNCION MAP PARA QUE ME PRODUZCA LOS DATAFRAME A GUARDAR ^^^^"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"fa728c0c-4dc9-49f2-a12b-69d182a29fff","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["dfVentasInternet=spark.read.format(\"jdbc\").option(\"url\", jdbcUrl).option(\"dbtable\", \"dbo.ventasinternet\").load()\n#INVOCO A LA TABLA VENTAS INTERNET DE SQL. DE ACA VOY A HACER UNA CONSULTA PARA CONSEGUIR EL STOCK OPTIMO ^^^^^^\ndfStockProductos=spark.read.format(\"jdbc\").option(\"url\", jdbcUrl).option(\"dbtable\", \"dbo.stockproductos\").load()\n\n\n#OBSERVACION\ndfVentasInternet.createOrReplaceTempView('vista_tabla')\ndfVentasInternet2=spark.sql('select sum(cantidad) as suma, cod_producto, cod_territorio, month(fechaenvio) as mes from vista_tabla group by month(fechaenvio), cod_producto, cod_territorio')\ndfVentasInternet2.createOrReplaceTempView('dfVentasInternet2')\ndfStockOptimo=spark.sql('select cod_producto, cod_territorio, avg(suma) as cantidad from dfVentasInternet2 group by cod_producto, cod_territorio order by cod_producto, cod_territorio')\n#creo tabla de stock optimo, que va a usarse de referencia en la distribucion de stock\n\n\n\n\ndfStockOptimo=dfStockOptimo.withColumn('cantidad', dfStockOptimo[\"cantidad\"].cast('integer'))\ndfStockOptimo.show()\n#uso finalmente una instancia del dataframe stock optimo para referenciarla en la siguiente funcion\n'''NOTA: esta celda puede ser innecesariamente densa, la intencion de todo esto es obtener solamente la tabla stock, pero en databricks no consegui trabajar con \nla sentencia CREATE VIEW de sql \"OBSERVACION\", si se pudiera hacer eso el codigo se puede optimizar'''"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1f30d654-3d4c-4e97-ab0f-6f43183364c2","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#con esta funcion, aplico la distribucion de stock a las sucursales usando como referencia el dataframe \"stock\"\n\n\ndef distribuir_stock(cod_prod,cantidad):\n    \n    #cod_producto es el producto en si mismo\n    #la cantidad es el stock que agregas de ese producto\n    #filtrado de tabla stock, quiero crearme un porcentaje de distribucion en tiempo real del codigo producto pedido\n    df=dfStockOptimo.filter(dfStockOptimo['cod_producto']==cod_prod)\n    \n    df1=dfStockProductos.filter(dfStockProductos['cod_producto']==cod_prod)\n    #CHEQUEO SI HAY STOCK EXISTENTE\n    \n    #-------------------------------------------------------------------------------------------------------------------------------\n    \n    #CASO 1: SI EL PRODUCTO TIENE REFERENCIA DE STOCK\n    if df.count() > 0: #si el dataframe es no nulo\n        df_auxiliar=df.select('cod_territorio','cantidad')\n        #transformacion a lista y parse a int\n        df_auxiliarDePandas=df_auxiliar.toPandas()\n        arrays=df_auxiliarDePandas.to_numpy().transpose().tolist() #esto sirve para usar las columnas como listas\n        \n        #arrays es una lista de listas, la primera son las sucursales que tienen el producto, la segunda es el stock ideal\n        sucursales=[int(i) for i in arrays[0]]\n        referencia_stock=[int(i) for i in arrays[1]]\n        \n        suma_cantidad=sum(referencia_stock)\n        todas_las_sucursales=[1,2,3,4,5,6,7,8,9,10]\n        \n        #cuando la cantidad de sucursales que registran ventas sea menor a 10, ejecuto esto que me completa el nÂ° de sucursales a 10\n        if len(sucursales)<10: \n            [todas_las_sucursales.remove(i) for i in sucursales] #luego de este bucle, del total de sucursales solo quedan las que no estaban inicialmente\n            for j in todas_las_sucursales:\n                referencia_stock.append(0) #las sucursales que no estaban inicialmente les agrego stock cero\n            porcentaje=list(map(lambda x: (x/suma_cantidad)*100,referencia_stock))\n            sucursales=sucursales+todas_las_sucursales\n        \n        #si las sucursales estan bien, simplemente aplico el porcentaje de distribucion\n        else:\n            porcentaje=list(map(lambda x: (x/suma_cantidad)*100,referencia_stock))\n        \n        #si la cantidad de stock es mayor a la cantidad de sucursales (10)\n        if cantidad>10:\n            stock_sucursal_auxiliar_1=[1,1,1,1,1,1,1,1,1,1]\n            #la primera reparticion de stock va a ser uniforme, esto hago arriba ^^^^\n            \n\n            stock_sucursal_auxiliar_2=list(map(lambda x: int((cantidad-10)*(x/100)),porcentaje))\n            #luego de la primera reparticion, continuo pero ahora con los porcentajes de venta por producto de cada sucursal ^^^^\n            \n            stock_sucursal=list(map(lambda x,y: x+y,stock_sucursal_auxiliar_1,stock_sucursal_auxiliar_2))\n            #unifico las dos reparticiones\n            \n            diferencia=cantidad-sum(stock_sucursal)\n            #al haber usado int en stock_sucursal_auxiliar_2 me redondea la cantidad para abajo (porcentajes usa decimales), el excedente se reparte\n\n            for i in range(len(stock_sucursal)):\n                x=stock_sucursal[i]+1\n                stock_sucursal.pop(i)\n                stock_sucursal.insert(i,x)\n                diferencia-=1\n                if diferencia==0:\n                    break\n        \n        #si la cantidad de stock es menor a la cantidad de sucursales, reparto de a 1 en las primeras sucursales\n        else:\n            stock_sucursal=[0,0,0,0,0,0,0,0,0,0]\n            for z in range(len(stock_sucursal)):\n                x=stock_sucursal[z]+1\n                stock_sucursal.pop(z)\n                stock_sucursal.insert(z,x)\n                cantidad-=1\n                if cantidad==0:\n                    break \n        \n        if df1.count()>0:\n            df_auxiliar2=df1.select('StockReal')\n            df_auxiliarDePandas2=df_auxiliar2.toPandas()\n            arrays1=df_auxiliarDePandas2.to_numpy().transpose().tolist() #esto sirve para usar las columnas como listas\n            stock_sucursal=list(map(lambda x,y: x+y,stock_sucursal,arrays1))\n            #ACA SUMO EL STOCK QUE AGREGO CON EL EXISTENTE \n                                \n            \n    #-------------------------------------------------------------------------------------------------------------------------------\n    \n    \n    #CASO 2: si el dataframe stock no tiene registros para crear la distribucion del stock, aplico la distribucion del total de ventas historico\n    else:\n        porcentajes=[14.639358483134,0.040200579734,0.031737299790,19.945835008022,0.059242959608,11.876097581418,9.438672957546,9.614286016384,22.745064849500,11.609504263182]\n        referencia_stock=[0,0,0,0,0,0,0,0,0,0]\n        sucursales=[1,2,3,4,5,6,7,8,9,10]\n        if cantidad>10:\n\n            stock_sucursal1=[1,1,1,1,1,1,1,1,1,1]\n            stock_sucursal2=list(map(lambda x: int((cantidad-10)*(x/100)),porcentajes))\n            stock_sucursal=list(map(lambda x,y: x+y,stock_sucursal1,stock_sucursal2))\n            diferencia=cantidad-sum(stock_sucursal)\n\n            for i in range(len(stock_sucursal)):\n                x=stock_sucursal[i]+1\n                stock_sucursal.pop(i)\n                stock_sucursal.insert(i,x)\n                diferencia-=1\n                if diferencia==0:\n                    break\n        else:\n            stock_sucursal=[0,0,0,0,0,0,0,0,0,0]\n            for i in range(len(stock_sucursal)):\n                x=stock_sucursal[i]+1\n                stock_sucursal.pop(i)\n                stock_sucursal.insert(i,x)\n                cantidad-=1\n                if cantidad==0:\n                    break  \n                                \n        if df1.count()>0:\n            df_auxiliar2=df1.select('StockReal')\n            df_auxiliarDePandas2=df_auxiliar2.toPandas()\n            arrays1=df_auxiliarDePandas2.to_numpy().transpose().tolist() \n            print(arrays1)\n            stock_sucursal=list(map(lambda x,y: x+y,stock_sucursal,arrays1[0]))\n            #ACA SUMO EL STOCK QUE AGREGO CON EL EXISTENTE \n            \n    datos={'Cod_Producto':[cod_prod,cod_prod,cod_prod,cod_prod,cod_prod,cod_prod,cod_prod,cod_prod,cod_prod,cod_prod],\n   'Cod_Sucursal':sucursales,\n   'StockOptimo':referencia_stock,\n   'StockReal':stock_sucursal}\n    df=pd.DataFrame(datos)\n    return spark.createDataFrame(df)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5ad3881a-9337-4e00-ab66-c4f428033bfa","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#esta operacion lo que hace es usar la funcion distribuir stock y las listas del json cargados del datalake, y con las variables creadas hago la distribucion automatica\nlista_dataframes=list(map(distribuir_stock,lista_cod_producto,lista_stock))\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"cf1dfede-c800-4d85-b1e1-53259fdfa0d7","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def upsertAzureSQL(df, azureStagingTable, azureSqlTargetTable, lookupColumns, deltaName):\n    targetTableAlias=\"stage\"\n    stagingTableAlias=\"stockproductos\"\n    \n    dfColumns=str(df.columns)\n    dfColumns=(((dfColumns.replace(\"'\",\"\")).replace(\"[\",\"\")).replace(\"]\",\"\")).replace(\" \",\"\")\n    \n    mergeStatement= \"MERGE \"+azureSqlTargetTable+\" as \"+targetTableAlias+\" USING \"+azureStagingTable+\" as \"+stagingTableAlias+\" ON (\"\n    \n    if (lookupColumns is not None or lookupColumns is len(lookupColumns)>0):\n        uniqueCols=lookupColumns.split(\"|\")\n        lookupStatement=\"\"\n        for lookupCol in uniqueCols:\n            lookupStatement=lookupStatement+targetTableAlias+\".\"+lookupCol+\" = \"+stagingTableAlias+\".\"+lookupCol+ \" and \"\n        \n    if deltaName is not None and len(deltaName)>0:\n        updateStatement=lookupStatement+stagingTableAlias+\".\"+deltaName+\" >= \"+targetTableAlias+\".\"+deltaName\n    else:\n        remove=\"and\"\n        reverse_remove=remove[::-1]\n        updateStatement=lookupStatement[::-1].replace(reverse_remove,\"\",1)[::-1]\n    if deltaName is not None and len(deltaName)>0:\n        updateStatement=updateStatement+\" and \"+targetTableAlias+\".\"+deltaName+\" < \"+stagingTableAlias+\".\"+deltaName\n    updateStatement=updateStatement+\") WHEN MATCHED THEN UPDATE SET \"\n    updateColumns=dfColumns.split(\",\")\n    for lookupCol in updateColumns:\n        updateStatement=updateStatement+targetTableAlias+\".\"+lookupCol+\" = \"+stagingTableAlias+\".\"+lookupCol+\", \"\n    remove=\",\"\n    reverse_remove=remove[::-1]\n    updateStatement=updateStatement[::-1].replace(reverse_remove,\"\",1)[::-1]+\";\"\n    \n    updateStatement=mergeStatement+updateStatement\n    \n    remove=\"and\"\n    reverse_remove=remove[::-1]\n    insertLookupStatement=lookupStatement[::-1].replace(reverse_remove,\"\",1)[::-1]+\")\"\n    \n    insertStatement=insertLookupStatement+\" WHEN NOT MATCHED BY TARGET THEN INSERT (\"+dfColumns.replace(\",\", \", \")+\") VALUES (\"\n    for lookupCol in updateColumns:\n        insertStatement=insertStatement+stagingTableAlias+\".\"+lookupCol+\", \"\n    remove=\",\"\n    reverse_remove=remove[::-1]\n    insertStatement=insertStatement[::-1].replace(reverse_remove,\"\",1)[::-1]+\");\"\n    insertStatement=mergeStatement+insertStatement\n    finalStatement=updateStatement+insertStatement\n    \n    df.write.format(\"jdbc\").option(\"url\", jdbcUrl).option(\"dbtable\", azureStagingTable).option(\"truncate\",\"true\").option(\"schemaCheckEnabled\",\"false\").mode(\"overwrite\").save()\n    \n    conn=pyodbc.connect('DRIVER={ODBC Driver 17 for Sql Server};'\n                        'SERVER='+AzureSQL+'.database.windows.net;'\n                        'DATABASE='+bacpac+';UID='+user+';'\n                        'PWD='+pss)\n    cursor=conn.cursor()\n    conn.autocommit=True\n    cursor.execute(finalStatement)\n    conn.close()\n    \n    return finalStatement"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"eeaa7759-4b2b-4d99-8025-c1ff92a5a443","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sh\ncurl https://packages.microsoft.com/keys/microsoft.asc | apt-key add -\ncurl https://packages.microsoft.com/config/ubuntu/16.04/prod.list > /etc/apt/sources.list.d/mssql-release.list \napt-get update\nACCEPT_EULA=Y apt-get install msodbcsql17\napt-get -y install unixodbc-dev\nsudo apt-get install python3-pip -y\npip3 install --upgrade pyodbc\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"bbfcfc1e-43ff-4130-a973-e6d0e502bd5b","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["list(map(upsertAzureSQL,lista_dataframes,\n         [\"Stage\" for i in range(len(lista_stock))],\n         [\"stockproductos\" for i in range(len(lista_stock))],\n         [\"Cod_Producto|Cod_Sucursal\" for i in range(len(lista_stock))],\n         [None for i in range(len(lista_stock))]))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"adb129e7-96ee-477e-a959-96aa288ea06d","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["upsertAzureSQL(dataframe, \"stage\", \"stockproductos\", \"Cod_Producto|Cod_Sucursal\", None)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"4fd45b21-f60e-462d-bf39-e240ab63617c","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["dfStage=spark.read.format(\"jdbc\").option(\"url\", jdbcUrl).option(\"dbtable\", \"dbo.stage\").load()\ndfStockProductos=spark.read.format(\"jdbc\").option(\"url\", jdbcUrl).option(\"dbtable\", \"dbo.StockProductos\").load()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"773054b1-45db-478d-a8e6-2a5876e60cc6","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c051daa0-4775-4e91-8751-bcef0dc850bb","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Distribucion sucursal y update","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":1244380783195857}},"nbformat":4,"nbformat_minor":0}
